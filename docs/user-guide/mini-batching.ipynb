{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its purest form, online machine learning encompasses models which learn with one sample at a time. This is the design which is used in `creme`.\n",
    "\n",
    "The main downside of single-instance processing is that it doesn't scale to big data. Indeed, processing one sample at a time means that we are able to use [vectorisation](https://www.wikiwand.com/en/Vectorization) and other computational tools that are taken for granted in batch learning. On top of this, processing a large dataset in `creme` essentially involves a Python `for` loop, which might be too slow for some usecases. However, this doesn't mean that `creme` is slow. In fact, for processing a single instance, `creme` is actually a couple of orders of magnitude faster than libraries such as scikit-learn, PyTorch, and Tensorflow. The reason why is because `creme` is designed from the ground up to process a single instance, whereas the majority of other libraries choose to care about batches of data. Both approaches offer different compromises, and the best choice depends on your usecase.\n",
    "\n",
    "In order to propose the best of both worlds, `creme` offers some limited support for mini-batch learning. Some of `creme`'s estimators implement `*_many` methods on top of their `*_one` counterparts. For instance, `preprocessing.StandardScaler` has a `learn_many` method as well as a `transform_many` method, in addition to `learn_one` and `transform_one`. Each mini-batch method takes as input a `pandas.DataFrame`. Supervised estimators also take as input a `pandas.Series` of target values. We choose to use `pandas.DataFrames` over `numpy.ndarrays` because of the simple fact that the former allows us to name each feature. This in turn allows us to offer a uniform interface for both single instance and mini-batch learning.\n",
    "\n",
    "As an example, we will build a simple pipeline that scales the data and trains a logistic regression. Indeed, the `compose.Pipeline` class can be applied to mini-batches, as long as each step is able to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import compose\n",
    "from creme import linear_model\n",
    "from creme import preprocessing\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LogisticRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use `datasets.Higgs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz (2.62 GB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Higgs dataset\n",
       "\n",
       "              Task  Binary classification                                                       \n",
       " Number of samples  11,000,000                                                                  \n",
       "Number of features  28                                                                          \n",
       "            Sparse  False                                                                       \n",
       "              Path  /Users/mhalford/creme_data/Higgs/HIGGS.csv.gz                               \n",
       "               URL  https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\n",
       "              Size  2.55 GB                                                                     \n",
       "        Downloaded  False                                                                       "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from creme import datasets\n",
    "\n",
    "dataset = datasets.Higgs()\n",
    "if not dataset.is_downloaded:\n",
    "    dataset.download()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to read the data in a mini-batch fashion is to use the `read_csv` from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = [\n",
    "    'target', 'lepton pT', 'lepton eta', 'lepton phi',\n",
    "    'missing energy magnitude', 'missing energy phi',\n",
    "    'jet 1 pt', 'jet 1 eta', 'jet 1 phi', 'jet 1 b-tag',\n",
    "    'jet 2 pt', 'jet 2 eta', 'jet 2 phi', 'jet 2 b-tag',\n",
    "    'jet 3 pt', 'jet 3 eta', 'jet 3 phi', 'jet 3 b-tag',\n",
    "    'jet 4 pt', 'jet 4 eta', 'jet 4 phi', 'jet 4 b-tag',\n",
    "    'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb'\n",
    "]\n",
    "\n",
    "for x in pd.read_csv(dataset.path, names=names, chunksize=8096, nrows=3e5):\n",
    "    y = x.pop('target')\n",
    "    y_pred = model.predict_proba_many(x)\n",
    "    model.learn_many(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with scikit-learn, you might be aware that [some](https://scikit-learn.org/stable/modules/computing.html#incremental-learning) of their estimators have a `partial_fit` method, which is similar to creme's `learn_many` method. Here are some advantages that creme has over scikit-learn:\n",
    "\n",
    "- We guarantee that creme's is just as fast, if not faster than scikit-learn. The differences are negligeable, but are slightly in favor of creme.\n",
    "- We take as input dataframes, which allows us to name each feature. The benefit is that you can add/remove/permute features between batches and everything will keep working.\n",
    "- Estimators that support mini-batches also support single instance learning. This means that you can enjoy the best of both worlds. For instance, you can train with mini-batches and use `predict_one` to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can check which estimators can process mini-batches programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline\n",
      "LinearRegression\n",
      "LogisticRegression\n",
      "StandardScaler\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import inspect\n",
    "\n",
    "def can_mini_batch(obj):\n",
    "    return hasattr(obj, 'learn_many')\n",
    "\n",
    "for module in importlib.import_module('creme').__all__:\n",
    "    for name, obj in inspect.getmembers(importlib.import_module(f'creme.{module}'), can_mini_batch):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because mini-batch learning isn't treated as a first-class citizen, some of the creme's functionalities require some work in order to play nicely with mini-batches. For instance, the objects from the `metrics` module have an `update` method that take as input a single pair `(y_true, y_pred)`. This might change in the future, depending on the demand.\n",
    "\n",
    "We plan to promote more models to the mini-batch regime. However, we will only be doing so for the methods that benefit the most from it, as well as those that are most popular. Indeed, `creme`'s core philosophy will remain to cater to single instance learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
